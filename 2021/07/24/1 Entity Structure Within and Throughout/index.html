<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Chenhs">





<title>Chenhs</title>



    <link rel="icon" href="/favicon.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        <!-- MathJax配置，可通过单美元符号书写行内公式等 -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
    "HTML-CSS": {
        preferredFont: "TeX",
        availableFonts: ["STIX","TeX"],
        linebreaks: { automatic:true },
        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50)
    },
    tex2jax: {
        inlineMath: [ ["$", "$"], ["\\(","\\)"] ],
        processEscapes: true,
        ignoreClass: "tex2jax_ignore|dno",
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
        equationNumbers: { autoNumber: "AMS" },
        noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } },
        Macros: { href: "{}" }
    },
    messageStyle: "none"
    });
</script>
<!-- 给MathJax元素添加has-jax class -->
<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>
<!-- 通过连接CDN加载MathJax的js代码 -->
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    


<meta name="generator" content="Hexo 5.4.0"></head>
<body>
    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">思录</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/Hi">About</a>
                
                    <a class="menu-item" href="/Lists">Lists</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">思录</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/Hi">About</a>
                
                    <a class="menu-item" href="/Lists">Lists</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
        <div class="main">
            <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    document.ready(
        function () {
            tocbot.init({
                tocSelector: '.tocbot-list',
                contentSelector: '.post-content',
                headingSelector: 'h1, h2, h3, h4, h5',
                collapseDepth: 1,
                orderedList: false,
                scrollSmooth: true,
            })
        }
    )

    function expand_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 6,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "collapse_toc()");
        b.innerHTML = "Collapse all"
    }

    function collapse_toc() {
        var b = document.querySelector(".tocbot-toc-expand");
        tocbot.init({
            tocSelector: '.tocbot-list',
            contentSelector: '.post-content',
            headingSelector: 'h1, h2, h3, h4, h5',
            collapseDepth: 1,
            orderedList: false,
            scrollSmooth: true,
        });
        b.setAttribute("onclick", "expand_toc()");
        b.innerHTML = "Expand all"
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title"></h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Chenhs</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">七月 24, 2021&nbsp;&nbsp;16:07:32</a>
                        </span>
                    
                    
                </div>
            
        </header>

        <div class="post-content">
            <h2 id="Snapshot"><a href="#Snapshot" class="headerlink" title="Snapshot"></a>Snapshot</h2><ul>
<li><p>模型名称：SSAN（<strong>S</strong>tructured <strong>S</strong>elf-<strong>A</strong>ttention <strong>N</strong>etwork）</p>
</li>
<li><p>论文来源：AAAI 2021 中科大 </p>
</li>
<li><p>面向任务：DocRE</p>
</li>
<li><p>方法分类：BERT-based</p>
</li>
<li><p>论文动机：着重关注于entity structure, 以往都是将这部分作为一个补充信息或者通过一个精心设计的图结构(delicately designed)引入entity structure。在基于graph的方法中，context reasoning 与 structure reasoning是分开的(isolation)，作者认为这样不合理。</p>
<blockquote>
<p>观点：The contextual representations cannot <strong>benefit from structure guidance</strong> in the first place. Thus, structural dependencies should be incorporated within the encoding network and throughout the overall system. 即在编码阶段融入结构依赖信息。</p>
</blockquote>
</li>
<li><p>指标分数：</p>
<ul>
<li><p>Ign F1:56.06;F1:58.41(BERT Base)   </p>
</li>
<li><p>Ign F1:63.78;F1:65.92 (+Adaptation预训练 )</p>
<blockquote>
<p>BERT Base并没有LSR指标好，Adaptiation思路可以借鉴；</p>
</blockquote>
</li>
</ul>
</li>
<li><p>快看速评：<del>动机来得比较奇怪，很好奇怎么会有这个动机，</del>大部分基于PLMs的模型往往都忽略了文档本身所带包含的结构依赖信息（such dependencies 能够反映在实体mention间丰富的交互），而只是从context的角度出发去更新实体表示，进而进行下一步的关系分类；或者基于图结构编码文本结构信息和 conextual 信息是</p>
</li>
<li><p>在不同的 module 使用的（如LSR）。作者认为自己的模型可以同时进行context reasoning和structure reasoning。</p>
</li>
<li><p>代码复现：<a target="_blank" rel="noopener" href="https://github.com/BenfengXu/SSAN">https://github.com/BenfengXu/SSAN</a></p>
</li>
</ul>
<h3 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h3><p><strong>SSAN核心在于formulate entity structure priors到attention biases,利用attention biass调整self-attention的propagation.</strong></p>
<div align="center">
        <img src="https://cdn.jsdelivr.net/gh/chenhaishun/test_pic@master/typora202103/21/104317-728900.png">  
    </a>
</div>

<ol>
<li><p>预定义mentions之间的关系：</p>
<ul>
<li>Co-occurrence structure: 两个mention出现在同一个sentence。</li>
<li>Coreference structure:两个mention指向同一实体。</li>
</ul>
<div align="center">
        <img src="https://cdn.jsdelivr.net/gh/chenhaishun/test_pic@master/typora202103/19/201139-22843.png">  
    </a>
</div>

<p>而对于非entity的words之间，若在同一sentence中，记为<em>intraNE</em>，在不同句子中记为<em>NA</em>；</p>
<p>The overall structure is formulated into an entity-centric adjacency matrix with its elements from a finite dependency set.</p>
<p>整个结构被公式化为以实体为中心的邻接矩阵，其所有元素来自一个有限的依赖集;</p>
<p>总计为6种结构关系：<em>{intra+coref, inter+coref, intra+relate, inter+relate, intraNE, NA}</em></p>
</li>
<li><p>设定 token sequence $x = (x_1,x_2,\dots,x_n)$ 以及 $S=\left\{s_{ij}\right\}$ , $s_{i j} \in$ <em>{intra+coref, inter+coref, intra+relate, inter+relate, intraNE, NA}</em>，根据输入$x$, 得到相应的query/key/value vector：</p>
<blockquote>
<p>实际编码</p>
</blockquote>
<script type="math/tex; mode=display">
\boldsymbol{q}_{i}^{l}=\boldsymbol{x}_{i}^{l} \boldsymbol{W}_{l}^{Q}, \boldsymbol{k}_{i}^{l}=\boldsymbol{x}_{i}^{l} \boldsymbol{W}_{l}^{K}, \boldsymbol{v}_{i}^{l}=\boldsymbol{x}_{i}^{l} \boldsymbol{W}_{l}^{V}</script><p>给定输入，计算<strong>unstructured attention score</strong>:</p>
<script type="math/tex; mode=display">
e_{i j}^{l}=\frac{\boldsymbol{q}_{i}^{l} \boldsymbol{k}_{j}^{l^{T}}}{\sqrt{d}}</script><p>与计算<strong>structured attention bias</strong>: </p>
<blockquote>
<p>To incorporate the discrete structure $s_{ij}$ conditioned on their contextualized query / key representation：</p>
</blockquote>
</li>
</ol>
<script type="math/tex; mode=display">
bias_{ij}^l=\frac{\operatorname{transformation}\left(\boldsymbol{q}_{i}^{l}, \boldsymbol{k}_{j}^{l}, s_{i j}\right)}{\sqrt{d}}</script><p>   Transformation Module:</p>
<ul>
<li><p>Biaffine Transformation</p>
<script type="math/tex; mode=display">
bias _{i j}^{l}=\boldsymbol{q}_{i}^{l} \boldsymbol{A}_{l, s_{i j}} \boldsymbol{k}_{j}^{l^{T}}+b_{l, s_{i j}}</script></li>
<li><p>Decomposed Linear Transformation</p>
<script type="math/tex; mode=display">
bias_{i j}^{l}=\boldsymbol{q}_{i}^{l} \boldsymbol{K}_{l, s_{i j}}^{T}+\boldsymbol{Q}_{l, s_{i j}} \boldsymbol{k}_{j}^{l^{T}}+b_{l, s_{i j}}</script><blockquote>
<p>$b_{l, s_{i j}}$: prior bias, independent to its context.</p>
</blockquote>
</li>
</ul>
<ol>
<li>计算在pre-specified relation下的概率<script type="math/tex; mode=display">
P_{r}\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{o}\right)=\operatorname{sigmoid}\left(\boldsymbol{e}_{s} \boldsymbol{W}_{r} \boldsymbol{e}_{o}\right)</script>Cross entropy loss：<script type="math/tex; mode=display">
L=\sum_{<s, o>} \sum_{r} CrossEntropy \left(P_{r}\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{o}\right), \bar{y}_{r}\left(\boldsymbol{e}_{s}, \boldsymbol{e}_{o}\right)\right)</script></li>
</ol>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><ul>
<li>如下,大部分情况下Biaffine Transformation较Decomposition Linear Transformation更好，总体效果在BERT作为PLMs下，效果还是不如LSR（   yyds！）</li>
</ul>
<div align="center">
        <img src="https://cdn.jsdelivr.net/gh/chenhaishun/test_pic@master/typora202103/20/170020-286198.png">  
    </a>
</div>

<ul>
<li><p>+Adaptation</p>
<ul>
<li><p>动机：Newly introduced transformation layer与 pretrained Transformer models 存在 distribution gap</p>
</li>
<li><p>做法：在finetuning on人工精标训练集前，在大量的远程监督数据集上先pretrain下。</p>
<blockquote>
<p>distribution gap 待考证 TODO-2</p>
</blockquote>
</li>
</ul>
</li>
<li><p>消融实验</p>
<ul>
<li><p>每一个预定的mentions 间关系都很重要，缺一不可</p>
</li>
<li><p>Bias Term中，在decomposed Linear Transformation下，key conditioned bias 较 query conditioned bias对性能影响更大。</p>
<blockquote>
<p>key vectors might be associated with more entity structure information ???</p>
<p>有待考究其具体意义表示 TODO</p>
</blockquote>
</li>
</ul>
</li>
<li><p>可视化Attentive Biases</p>
<ul>
<li>底层layer受structured  bias影响较小；在顶层layer中，structural priors开始发力；</li>
<li>其中，Inter+coref significantly positive,也符合人类常识；</li>
<li>下图的右侧图中，bias施加在底层的影响小于对顶层的影响。</li>
</ul>
</li>
</ul>
<div align="center">
        <img src="https://cdn.jsdelivr.net/gh/chenhaishun/test_pic@master/typora202103/22/101503-711635.png">  
    </a>
</div>

<h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><ol>
<li><p>文献阅读：</p>
<p>[1] Simultaneously Self-Attending to All Mentions for Full-Abstract Biological Relation Extraction # multi-instance learning</p>
<p>[2] Self-Attention with Relative Position Representations 作者称此篇启发了这篇工作</p>
<p>[3] Transformer-XL: Attentive Language Models beyond a Fixed-Length Context</p>
<p>[4] Extracting Multiple-Relations in OnePass with Pre-Trained Transformers.</p>
</li>
<li><p>Distribution gap 怎么减小这类gap?</p>
</li>
<li><p>transformer中三个向量的意义考究。</p>
</li>
</ol>
<p> $A_{l,s_{i,j}}$或者$K_{l,s_{i,j}}, Q_{l,s_{i,j}}$,</p>
<p>对于定义的5种依赖关系，随机初始化5组可学习的$A$或者$K, Q$,其分别对应于五种依赖关系（NA关系不建模？），对于特定的token $i$, token$j$，其带给原本attention score的偏差根据不同的依赖关系不同（体现在于计算bias时用的是哪个可学习$A$或者$K, Q$）</p>
<p>师兄 如果每一transfomer layer中计算bias的A或者K，Q都不同，那初始化就不止五组A或K，Q了，而是5*l组A或K，Q了（我是不是哪里理解错了）</p>
<p>是啊，每一层都有新的AKQ</p>
<p>总觉得是不是应该只有5组 A,K,Q，每层共享会不会好一些呢，这样这5组A,K,V就唯一表示了五组依赖关系….</p>
<p>但是transformer每一层的representation是不同的，AKQ模块需要和每一层的hidden representation的k和q交互，是自适应的，layer 0的k,q和layer11的k,q不在同一个表示空间里，所以这里也引入了不同的AKQ模块去拟合它们</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Chenhs</span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>Permalink:</span>
                        <span><a href="http://example.com/2021/07/24/1%20Entity%20Structure%20Within%20and%20Throughout/">http://example.com/2021/07/24/1%20Entity%20Structure%20Within%20and%20Throughout/</a></span>
                    </p>
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                
                     <p class="copyright-item">
                         <span>Slogan:</span>
                         <span>😀😁😂🤣😃😄😅😆</span>
                     </p>
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
            
            <a class="next" rel="next" href="/2021/07/24/1%20Entity%20and%20Evidence%20Guided%20Relation%20Extraction%20for%20DocRED/"></a>
            
        </section>


    </article>
</div>







        </div>
        <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Chenhs | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a>
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<span class="site-uv">
    总访客量:
    <i class="busuanzi-value" id="busuanzi_value_site_uv"></i>
</span>&nbsp;


<span class="site-pv">
    总访问量:
    <i class="busuanzi-value" id="busuanzi_value_site_pv"></i>
</span>

</span>
    </div>
</footer>

    </div>
</body>
</html>
